---

### **ch6 코드 종합 설명**

이 코드는 MongoDB를 단일 서버(Standalone)로 운영하는 것을 넘어, **데이터의 안정성과 서비스의 무중단 운영**을 보장하기 위한 **Replica Set(복제 세트)**을 구축하는 과정을 다룹니다. 여러 대의 서버가 데이터를 실시간으로 복제하고, 장애 발생 시 자동으로 대처하는 방법을 실습을 통해 배웁니다.

가장 좋은 비유는 **'주연 배우와 스턴트 배우'**입니다.

*   **Primary (주연 배우)**: 모든 촬영(쓰기 작업)을 담당합니다. 관객(클라이언트)은 주연 배우에게만 대본(데이터)을 전달할 수 있습니다.
*   **Secondary (스턴트 배우)**: 주연 배우의 모든 연기(데이터)를 실시간으로 똑같이 따라 하며(복제) 대기합니다. 평소에는 간단한 장면(읽기 작업)을 대신 촬영할 수 있습니다.
*   **응급 상황 (Failover)**: 만약 주연 배우가 아프면(서버 다운), 대기하던 스턴트 배우 중 한 명이 즉시 주연 배우 역할을 이어받아(Primary로 승격) 촬영이 중단되지 않도록 합니다.

이 실습을 통해 우리는 이 '영화 촬영팀'을 직접 구성하고, 응급 상황을 시뮬레이션하여 어떻게 시스템이 자동으로 복구되는지 눈으로 확인하게 됩니다.

---

### **Part 1: 촬영장 준비 - Replica Set 환경 구축**

여러 대의 서버(DB 인스턴스)를 띄우기 위한 사전 준비 과정입니다.

#### **1. 코드, 문법 및 개별 설명**

```bash
# 각 서버가 사용할 데이터 저장 폴더 생성 (Windows 명령어)
mkdir c:\db\data\rs0-0
mkdir c:\db\data\rs0-1
mkdir c:\db\data\rs0-2

# 1번 서버 실행
mongod --replSet rs0 --port 27018 --bind_ip 0.0.0.0 --dbpath c:\db\data\rs0-0

# 2번 서버 실행
mongod --replSet rs0 --port 27019 --bind_ip localhost --dbpath c:\db\data\rs0-1

# 3번 서버 실행
mongod --replSet rs0 --port 27020 --bind_ip localhost --dbpath c:\db\data\rs0-2
```

- `mkdir`: 각 MongoDB 인스턴스가 사용할 고유한 데이터 저장 공간(폴더)을 만듭니다. 실제 운영 환경에서는 이 폴더들이 각기 다른 물리적 서버에 위치하게 됩니다.
- `mongod`: MongoDB 서버를 실행하는 명령어입니다.
- `--replSet rs0`: **가장 중요한 옵션입니다.** 이 옵션은 각 `mongod` 인스턴스에게 "너는 'rs0'이라는 팀(Replica Set)의 멤버야"라고 소속을 알려줍니다. 이 이름이 동일해야 서로를 팀원으로 인식할 수 있습니다.
- `--port`: 각 서버가 통신할 포트 번호를 지정합니다. 한 컴퓨터에서 여러 서버를 실행해야 하므로 충돌하지 않도록 다른 포트 번호를 할당합니다.
- `--dbpath`: 각 서버가 데이터를 저장하고 읽을 폴더를 지정합니다. 위에서 만든 폴더와 짝을 이룹니다.
- `--bind_ip`: 서버에 접속을 허용할 IP 주소를 설정합니다. `localhost`(또는 `127.0.0.1`)는 해당 컴퓨터 내부에서의 접속만 허용하고, `0.0.0.0`은 모든 외부 IP에서의 접속을 허용합니다. 팀 리더(조장) 서버는 다른 팀원들이 접속해야 하므로 `0.0.0.0`으로 설정하는 경우가 많습니다.

#### **2. 해당 설명**

이 단계는 실제로는 3대의 서버를 준비하는 것과 같습니다. 각 서버는 'rs0'라는 동일한 팀 이름을 갖고 있지만, 각자의 집(dbpath)과 연락처(port)는 다릅니다. 이제 3명의 배우가 각자의 분장실에서 촬영 준비를 마친 상태입니다.

---

### **Part 2: 촬영팀 결성 - Replica Set 초기화 및 상태 확인**

개별적으로 실행된 서버들을 하나의 공식적인 팀으로 묶는 과정입니다.

#### **1. 코드, 문법 및 개별 설명**

```javascript
// 1번 서버(27018)에 접속하여 팀 구성을 지시
mongosh --port 27018

// Replica Set 초기화 및 멤버 구성
rs.initiate(
{
  _id : "rs0", // 우리 팀의 공식 명칭
  members: [ // 우리 팀의 멤버 목록
    {_id:0, host:"localhost:27018"}, // 0번 멤버는 27018 서버
    {_id:1, host:"localhost:27019"}, // 1번 멤버는 27019 서버
    {_id:2, host:"localhost:27020"}  // 2번 멤버는 27020 서버
  ]
})

// 상태 확인 명령어
rs.status() // 상세 정보
db.isMaster() // 요약 정보 (누가 Primary인지)
```

- `mongosh --port 27018`: 3대의 서버 중 하나에 접속하여 초기 설정을 진행합니다. 아무 서버에나 접속해도 되지만, 보통 첫 번째 서버에 접속하여 시작합니다.
- `rs.initiate()`: **Replica Set을 공식적으로 생성하는 명령어입니다.** 이 명령을 실행한 서버가 최초의 **Primary(주연 배우)**가 됩니다.
  - `_id`: `--replSet`에서 지정한 팀 이름과 동일해야 합니다.
  - `members`: 함께 팀을 이룰 모든 서버의 주소 목록을 알려줍니다.
- `rs.status()`: 현재 Replica Set의 전체적인 상태를 아주 상세하게 보여줍니다. 각 멤버의 역할(Primary/Secondary), 건강 상태(health), 복제 지연 시간(lag) 등 운영에 필요한 모든 정보가 담겨있습니다.
- `db.isMaster()`: 현재 접속한 서버의 역할과 전체 멤버 목록 등 핵심 정보만 간략하게 보여주는 유용한 명령어입니다.

#### **2. 해당 설명**

`rs.initiate()`를 실행하는 순간, 뿔뿔이 흩어져 있던 서버들이 `rs0`라는 이름 아래 하나의 팀으로 묶입니다. 시스템은 멤버들 간의 통신을 통해 자동으로 역할을 분담하여 최초의 Primary를 선출하고 나머지를 Secondary로 지정합니다. 이제 공식적인 '영화 촬영팀'이 결성되었습니다.

---

### **Part 3: 실시간 연기 지도 - 복제(Replication) 테스트**

Primary에서 발생한 데이터 변경이 Secondary에 실시간으로 복제되는지 확인합니다.

#### **1. 코드, 문법 및 개별 설명**

```javascript
// 1. Primary(27018)에 접속된 상태에서 데이터 삽입 (쓰기 작업)
db.replicationTest2.insertOne({hello2: 'world2'})

// 2. Secondary(27019)에 접속
mongosh --port 27019

// 3. Secondary에서 데이터 조회 시도 -> 에러 발생!
db.replicationTest2.find()

// 4. Secondary에서 읽기를 허용하도록 설정
rs.slaveOk() // 또는 최신 버전에서는 rs.secondaryOk()
// 이제 다시 find()를 실행하면 데이터가 보입니다.
```

- **Primary에서의 쓰기**: 데이터 생성, 수정, 삭제(`insert`, `update`, `delete`)와 같은 쓰기 작업은 **오직 Primary 멤버에서만** 가능합니다. 이는 데이터의 일관성을 유지하기 위한 매우 중요한 규칙입니다.
- **Secondary에서의 읽기 에러**: 기본적으로 Secondary에서는 데이터를 읽을 수 없도록 막혀 있습니다. 왜냐하면 아주 짧은 순간이지만 Primary의 데이터와 미세한 시차(복제 지연)가 있을 수 있기 때문입니다. MongoDB는 사용자에게 "이 데이터는 100% 최신이 아닐 수도 있습니다"라는 사실을 인지시키기 위해 기본적으로 읽기를 막아둡니다.
- `rs.slaveOk()`: "복제 지연이 있을 수 있다는 점을 인지했으며, 그럼에도 불구하고 데이터를 읽겠습니다"라고 명시적으로 허용해주는 명령어입니다. 이 명령을 실행한 셸 세션에서만 읽기가 가능해집니다.

#### **2. 해당 설명**

주연 배우(Primary)가 새로운 대사("hello2: world2")를 하자마자, 스턴트 배우(Secondary)들도 즉시 그 대사를 외웁니다(복제). 우리가 스턴트 배우에게 대사를 물어봤을 때, 처음에는 "저는 주연이 아니라서요..."라며 거절하지만(에러), "괜찮으니 아는 대로 말해주세요"(`rs.slaveOk()`)라고 하면 외우고 있던 대사를 말해주는(데이터 조회 성공) 과정입니다.

#### **3. 실무 활용 사례**

읽기 작업이 매우 많은 서비스(예: 블로그, 뉴스 사이트)에서는 모든 읽기 요청을 Primary에 집중시키면 부하가 심해집니다. 이럴 때, 데이터의 실시간성이 아주 중요하지 않은 조회 작업(예: 통계, 분석, 백오피스 대시보드)은 `rs.slaveOk()`를 설정하고 Secondary 서버들로 분산시켜 Primary의 부하를 덜어줄 수 있습니다. 이를 **'읽기 분산(Read Distribution)'**이라고 합니다.

---

### **Part 4: 위기 상황 대처 - 자동 장애 복구(Failover)와 수동 제어**

Primary 서버에 문제가 생겼을 때 Replica Set이 어떻게 대처하는지, 그리고 관리자가 수동으로 역할을 변경하는 방법을 배웁니다.

#### **1. 코드, 문법 및 개별 설명**

```javascript
// 1. 현재 Primary를 강제로 Secondary로 강등시킴 (장애 상황 시뮬레이션)
rs.stepDown();

// 2. 특정 멤버의 Primary 승격 우선순위 높이기
cfg = rs.conf(); // 현재 Replica Set 설정을 변수에 저장
cfg.members[0].priority = 2; // 0번 멤버의 우선순위를 2로 높임 (기본값은 1)
rs.reconfig(cfg); // 변경된 설정 적용

// 3. 특정 Secondary가 Primary가 되지 못하도록 동결
rs.freeze(0); // 0초 동안 동결 (일시적)
```

- `rs.stepDown()`: 현재 Primary에게 "주연 역할을 내려놓으세요"라고 명령합니다. 그러면 Primary는 스스로 Secondary로 강등되고, 나머지 Secondary 멤버들 사이에서 **'선거(Election)'**가 시작되어 새로운 Primary를 선출합니다. 서버를 재부팅하거나 점검해야 할 때, 서비스 중단 없이 역할을 안전하게 넘기기 위해 사용합니다.
- `priority` (우선순위): 선거 시 더 높은 우선순위 값을 가진 멤버가 Primary로 선출될 확률이 높습니다. 예를 들어, 더 고사양 서버에 높은 `priority`를 부여하여 항상 최고의 서버가 Primary가 되도록 유도할 수 있습니다. `priority`가 0인 멤버는 절대 Primary가 될 수 없습니다.
- `rs.freeze()`: 유지보수 등의 이유로 특정 Secondary 멤버가 잠시 동안 Primary 선거에 참여하지 못하도록 막는 명령어입니다.

#### **2. 심화 내용 (선거는 어떻게 이루어지는가?)**

Primary 서버와 연결이 끊어지면, Secondary들은 서로 통신하며 새로운 리더를 뽑는 선거를 시작합니다. 이때 가장 중요한 투표 기준은 **'누가 가장 최신 데이터를 가지고 있는가'**입니다. 가장 뒤처지지 않고 데이터를 복제한 멤버가 1순위 후보가 되며, 만약 그런 후보가 여러 명이면 `priority`가 높은 멤버가 당선됩니다. 이 모든 과정은 수 초 내에 자동으로 이루어지며, 이를 **'자동 장애 복구(Automatic Failover)'**라고 합니다.

---

### **Part 5: 복제의 비밀 - Oplog(오플로그)**

데이터 복제가 어떤 원리로 이루어지는지 내부 동작을 들여다봅니다.

#### **1. 코드, 문법 및 개별 설명**

```javascript
// oplog 컬렉션 접속
use local;
// 가장 최근의 oplog 1개 조회
db.oplog.rs.find().limit(1).pretty();
```

- `oplog (Operations Log)`: Replica Set의 심장과도 같은 존재입니다. Primary에서 발생하는 모든 데이터 변경 작업(insert, update, delete 등)은 실제 데이터에 적용되기 전에, 먼저 이 `oplog`라는 특별한 컬렉션에 기록됩니다.
- **복제 과정**: Secondary들은 이 `oplog`를 계속 감시하고 있다가, 새로운 로그가 기록되면 그것을 가져와 자신의 데이터에 그대로 재현(replay)합니다. 즉, 데이터 자체를 복사하는 것이 아니라, **'데이터를 변경시킨 명령어'**를 복사해서 실행하는 방식입니다. 이는 매우 효율적입니다.
- `local` 데이터베이스: `oplog`는 MongoDB 시스템이 내부적으로 사용하는 `local`이라는 특수한 데이터베이스 안에 저장되어 있습니다.

#### **2. 해당 설명**

`oplog`는 주연 배우(Primary)의 모든 행동이 순서대로 기록된 **'촬영 일지'**와 같습니다. 스턴트 배우들(Secondaries)은 이 촬영 일지를 계속 받아보면서, 주연 배우가 했던 행동을 시간 순서대로 똑같이 따라 하는 것입니다. 이 촬영 일지만 있으면 누구든 주연 배우의 역할을 이어받을 수 있습니다.

---

### **Part 6: 기타 관리 명령어**

Replica Set 운영에 도움이 되는 추가적인 명령어들입니다.

#### **1. 코드, 문법 및 개별 설명**

```bash
# 각 멤버들의 복제 상태 정보 출력
rs.printSlaveReplicationInfo()

# 조장(Primary)과 조원(Secondary) 간의 네트워크 연결 확인
# Windows cmd에서 실행
ping 10.100.201.x

# MongoDB 설정 파일 (Windows)
# 설치 경로의 /bin/mongod.cfg 파일에서 bind_ip 등 다양한 옵션 영구 설정 가능
```

- `rs.printSlaveReplicationInfo()`: 각 Secondary가 Primary의 `oplog`를 어디까지 따라왔는지, 얼마나 지연되고 있는지 등을 간략하게 보여줍니다.
- `ping`: 데이터베이스 문제가 아니라, 서버 간의 기본적인 네트워크 연결 자체에 문제가 있는지 확인할 때 사용하는 가장 기초적인 명령어입니다.
- `mongod.cfg`: 매번 `mongod`를 실행할 때마다 긴 옵션을 입력하는 대신, 이 설정 파일에 옵션들을 미리 기록해두면 간단하게 서버를 실행하고 관리할 수 있습니다. 운영 환경에서는 대부분 설정 파일을 사용합니다.

---

**Sharding(샤딩)**

---

### **ch6-2 코드 종합 설명**

이 코드는 단일 서버나 복제 세트(Replica Set)만으로는 감당할 수 없는 **초대용량의 데이터와 트래픽**을 처리하기 위해 MongoDB 클러스터를 구축하는 **Sharding(샤딩)** 과정을 다룹니다.

이전의 Replica Set 비유(주연 배우와 스턴트 배우)가 **'안정성'**을 위한 것이었다면, 샤딩은 **'규모의 확장'**을 위한 것입니다. 가장 좋은 비유는 **'거대한 국립 도서관 시스템'**입니다.

- **하나의 거대한 도서관 (Standalone/Replica Set의 한계)**: 모든 책(데이터)을 한 건물에 보관하면, 책이 너무 많아져서 보관할 공간이 부족해지고(Disk 한계), 책을 찾으러 오는 사람이 너무 많아 도서관 입구가 마비됩니다(CPU/RAM/Network 한계).

- **샤딩된 도서관 시스템 (Sharded Cluster)**:
  - **Shard (샤드 - 분관 도서관)**: 책을 주제별로 나누어 여러 개의 분관 도서관에 분산 보관합니다. '과학 분관', '문학 분관', '역사 분관'처럼 말이죠. 각 분관은 책(데이터)의 일부만 책임지므로 부담이 줄어듭니다.
  - **Config Server (설정 서버 - 중앙 도서 정보 시스템)**: 어떤 책이 어느 분관에 있는지 모든 **메타데이터(위치 정보)**를 관리하는 중앙 컴퓨터 시스템입니다. "『코스모스』는 과학 분관 3층에 있다"는 정보를 저장합니다.
  - **Mongos (몽고스 - 중앙 안내 데스크/라우터)**: 도서관 이용객(클라이언트)이 가장 먼저 찾아오는 곳입니다. 이용객이 "『코스모스』를 빌려주세요"라고 요청하면, Mongos는 설정 서버에 물어본 뒤, 이용객을 '과학 분관'으로 안내하는 역할을 합니다. **이용객은 분관의 존재를 알 필요 없이 오직 중앙 안내 데스크와만 소통하면 됩니다.**

이 실습을 통해 우리는 이 거대한 '국립 도서관 시스템'을 처음부터 끝까지 직접 설계하고 구축하게 됩니다.

---

### **Part 1: 중앙 도서 정보 시스템 구축 (Config Server Replica Set)**

모든 데이터의 위치 정보(메타데이터)를 저장할 '두뇌'를 만드는 과정입니다. 이 서버가 다운되면 전체 시스템이 마비되므로, **반드시 Replica Set으로 구성하여 고가용성을 확보해야 합니다.**

#### **1. 코드, 문법 및 개별 설명**

````bash
# 1. 설정 서버들이 사용할 데이터 폴더 생성
mkdir c:\db2\data\config0-0 ...

# 2. 3대의 설정 서버를 각각 실행
mongod --configsvr --replSet config0 --port 27018 ...```

*   `--configsvr`: 이 `mongod` 인스턴스의 역할을 **'설정 서버'**로 지정하는 매우 중요한 옵션입니다.
*   `--replSet config0`: 설정 서버들도 안정성을 위해 'config0'라는 이름의 Replica Set으로 묶어줍니다.

```javascript
// 3. 설정 서버 중 하나에 접속하여 Replica Set 초기화
mongosh --port 27018

// 4. 초기화
rs.initiate({
  _id : "config0",
  configsvr: true, // 이 Replica Set이 설정 서버용임을 명시
  members: [ ... ]
})
````

- `configsvr: true`: `rs.initiate()` 시 이 옵션을 `true`로 설정하여, 이 Replica Set이 일반 데이터용이 아닌 **샤드 클러스터의 메타데이터를 저장하는 특별한 목적**을 가짐을 알려줍니다.

#### **2. 해당 설명**

클러스터의 두뇌이자 심장인 설정 서버를 구축했습니다. 이제 이 서버는 앞으로 추가될 '분관 도서관(Shard)'들의 정보를 기록할 준비를 마쳤습니다. 이 과정은 샤딩 클러스터 구축의 가장 첫 번째 단계입니다.

---

### **Part 2: 분관 도서관 설립 (Shard Replica Set)**

실제 데이터가 저장될 여러 개의 '분관(Shard)'을 만드는 과정입니다. 각 Shard 또한 데이터 보호와 고가용성을 위해 **반드시 Replica Set으로 구성하는 것이 상용 환경의 표준**입니다.

#### **1. 코드, 문법 및 개별 설명**

```bash
# 1. 첫 번째 샤드(shard0)가 사용할 데이터 폴더 생성
mkdir c:\db2\data\shard0-0 ...

# 2. 샤드 서버들 실행
mongod --shardsvr --replSet shard0 --port 27021 ...
```

- `--shardsvr`: 이 `mongod` 인스턴스의 역할을 **'샤드 서버'**로 지정하는 옵션입니다. 이 서버가 데이터의 일부를 저장할 '분관'임을 알려줍니다.

```
javascript
// 3. 샤드 중 하나에 접속하여 Replica Set 초기화
mongosh --port 27021

// 4. 초기화
rs.initiate({
  _id : "shard0",
  members: [
    {_id:0, host:"localhost:27021"},
    {_id:1, host:"localhost:27022"},
    {_id:2, host:"localhost:27023", arbiterOnly: true} // Arbiter 멤버
  ]
})
```

- `arbiterOnly: true`: 이 멤버는 **'Arbiter(중재자)'** 역할을 합니다. Arbiter는 데이터를 저장하지 않으면서 Replica Set의 선거(Election)에 투표권만 행사하는 특별한 멤버입니다. Primary와 Secondary가 1:1로 남아 의견이 갈릴 때(Split Brain), Arbiter가 캐스팅보트 역할을 하여 과반수(Quorum)를 형성하고 새로운 Primary를 선출할 수 있도록 돕습니다. 실제 데이터를 저장하지 않으므로 적은 리소스로 홀수 개의 투표권을 확보할 때 유용합니다.

#### **2. 해당 설명**

이 코드에서는 `shard0`, `shard1`, `shard2`라는 이름의 분관 도서관 3개를 각각 설립했습니다. 각 분관은 자체적인 Replica Set(주연 배우와 스턴트 배우)을 갖추고 있어 안정적으로 운영될 준비가 되었습니다. 이제 독립적으로 운영되던 중앙 정보 시스템과 분관들을 하나로 연결할 차례입니다.

---

### **Part 3: 중앙 안내 데스크 오픈 (Mongos 라우터 실행 및 클러스터 통합)**

모든 요청을 받아 적절한 Shard로 안내해 줄 '중앙 안내 데스크(Mongos)'를 열고, 지금까지 만든 모든 구성 요소를 하나의 거대한 클러스터로 묶는 과정입니다.

#### **1. 코드, 문법 및 개별 설명**

````bash
# Mongos 실행
mongos --configdb config0/localhost:27018 --port 20000```

*   `mongos`: MongoDB 샤드 클러스터의 **'쿼리 라우터(Query Router)'**를 실행하는 명령어입니다.
*   `--configdb config0/localhost:27018`: **Mongos에게 '두뇌(설정 서버)'의 위치를 알려주는 가장 중요한 옵션입니다.** `config0`라는 Replica Set의 멤버 중 하나인 `localhost:27018`에 접속하여 전체 클러스터의 메타데이터 정보를 얻으라는 의미입니다.
*   `--port 20000`: 이제부터 모든 클라이언트와 개발자는 **이 Mongos 포트(20000)로만 접속**해야 합니다.

```javascript
// Mongos에 접속
mongosh --port 20000

// Mongos에 Shard(분관)들을 공식적으로 등록
sh.addShard("shard0/localhost:27021")
sh.addShard("shard1/localhost:27031")
sh.addShard("shard2/localhost:27041")

// 등록된 Shard 목록 확인
use config
db.shards.find()
````

- `sh.addShard()`: 현재 클러스터에 새로운 Shard를 추가하는 명령어입니다. 이 명령을 실행하면 Mongos가 설정 서버에 "이제부터 shard0, shard1, shard2가 우리 시스템의 공식 멤버입니다"라고 기록합니다.

#### **2. 해당 설명**

드디어 모든 조각이 맞춰졌습니다. 중앙 안내 데스크(Mongos)가 문을 열었고, 어느 분관(Shard)이 운영 중인지 파악했습니다. 이제 클라이언트는 20000번 포트로 접속하여 마치 하나의 거대한 데이터베이스를 사용하는 것처럼 데이터를 요청할 수 있습니다.

---

### **Part 4: 책 정리 규칙 정하기 (컬렉션 샤딩)**

데이터를 어떤 기준으로 나누어 각 Shard에 저장할지 규칙을 정하고 적용하는, 샤딩의 핵심 과정입니다.

#### **1. 코드, 문법 및 개별 설명**

```javascript
// 1. 'test2345' 데이터베이스에서 샤딩을 사용하겠다고 선언
sh.enableSharding("test2345");

// 2. 'test2345' 데이터베이스의 'test' 컬렉션을 'x' 필드를 기준으로 샤딩
sh.shardCollection("test2345.test", { x: "hashed" });
```

- `sh.enableSharding("데이터베이스이름")`: 특정 데이터베이스 단위로 샤딩 기능을 활성화합니다.
- `sh.shardCollection("DB.컬렉션", { 샤드키: "방식" })`: **샤딩의 핵심 명령어입니다.**
  - **Shard Key (샤드 키)**: 데이터를 분산시키는 기준이 되는 필드입니다. 예제에서는 `x` 필드를 샤드 키로 지정했습니다. 좋은 샤드 키를 선택하는 것이 샤딩 성능의 90%를 좌우합니다.
  - **Hashed Sharding (`"hashed"`)**: 샤드 키의 값을 해시(hash) 함수에 넣어 나온 결과값을 기준으로 데이터를 분산시킵니다. 데이터가 여러 Shard에 매우 균등하게 분산되는 효과가 있어 쓰기 작업이 많은 경우에 유리합니다. 마치 책 제목을 암호화해서 나온 숫자를 기준으로 아무 분관에나 골고루 배치하는 것과 같습니다.
  - (참고) **Ranged Sharding**: 샤드 키의 값 범위를 기준으로 데이터를 분산시킵니다. (예: `x`가 1~1000은 shard0, 1001~2000은 shard1에 저장). 특정 범위의 데이터를 함께 조회하는 경우 매우 효율적입니다.

#### **2. 심화 내용 (좋은 샤드 키의 조건)**

- **높은 카디널리티(Cardinality)**: 샤드 키가 가질 수 있는 고유한 값의 종류가 많아야 합니다. (예: `사용자 ID`, `상품 ID`)
- **낮은 빈도(Frequency)**: 특정 값에 데이터가 몰리지 않아야 합니다. (예: '상태' 필드에 '정상' 값이 99%라면 좋지 않은 키)
- **변경되지 않는 값**: 샤드 키 값은 한번 정해지면 변경할 수 없습니다.

---

### **Part 5: 실제 데이터 분산 확인**

데이터를 삽입하고, 실제로 여러 Shard에 나뉘어 저장되었는지 확인합니다.

#### **1. 코드, 문법 및 개별 설명**

```javascript
// 1. 테스트 데이터 12,000개 삽입
use test2345
for(var i = 1; i<12001; i++){
  db.test.insertOne({x:i, ename:i, sal:100})
}

// 2. 데이터 분산 상태 통계 확인 (Mongos에서 실행)
db.test.getShardDistribution()

// 3. 각 Shard에 직접 접속하여 저장된 문서 개수 확인
// shard0 (27021 포트)에서: db.test.count() -> 3468개
// shard1 (27031 포트)에서: db.test.count() -> N개
// shard2 (27041 포트)에서: M개
```

- `db.test.getShardDistribution()`: 각 Shard에 데이터와 청크(Chunk, 데이터의 최소 분산 단위)가 얼마나 분포되어 있는지 통계를 보여줍니다.
- **직접 접속하여 확인**: Mongos(20000 포트)로 접속하면 `db.test.count()`는 당연히 12000개가 나옵니다. 하지만 각 Shard(27021, 27031, 27041 포트)에 직접 접속하여 확인해보면, 전체 데이터가 아닌 일부 데이터만 저장되어 있음을 알 수 있습니다. **이는 샤딩이 성공적으로 이루어졌다는 가장 확실한 증거입니다.**

#### **2. 해당 설명**

12,000권의 책을 도서관 시스템에 기증했더니, Mongos(중앙 안내 데스크)는 'x'라는 샤드 키 규칙(`hashed`)에 따라 책들을 각 분관(Shard)에 골고루 나누어 보냈습니다. '과학 분관'에 가보니 약 3,468권이 있고, 나머지 책들은 다른 분관에 나뉘어 보관된 것을 확인함으로써, 우리의 거대한 도서관 시스템이 성공적으로 운영되고 있음을 증명했습니다.
